#
# Copyright 2020-2021 Enflame. All Rights Reserved.
#
from typing import Union
import torch
import torch_gcu
from torch_gcu.core.device import _device_t, _get_device_index, _check_device_valid
from torch_gcu.core.model import _is_gcu_tensor_list_or_tuple, _is_gcu_tensor, _assert_type_valid

__all__ = ['dump_hbm_alloc_info', 'dump_glob_hbm_use_info',
           'add_trace_point', 'metrics_report', 'get_graph', 'get_hbm_alloc_size']


def dump_hbm_alloc_info(device: _device_t = None):
    """Dump hbm info allocd by torch_gcu

    Args:
      device_id : device id, must in [0, device_count-1].
                  If device is None, use current device
    """
    device_id = _get_device_index(device, True)
    _check_device_valid(device_id)
    return torch_gcu._GCUC._dump_hbm_alloc_info(device_id)


def dump_glob_hbm_use_info(device: _device_t = None):
    """Dump global hbm info on GCU

    Args:
      device_id : device id, must in [0, device_count-1].
                  If device is None, use current device
    """
    device_id = _get_device_index(device, True)
    _check_device_valid(device_id)
    return torch_gcu._GCUC._dump_glob_hbm_use_info(device_id)


def add_trace_point(trace_info: str = '', sleep_time: float = 1):
    """Add a host trace point for debug.
       You can see it in profile data generated by Tops profiler tool

    Args:
      trace_info: information show in profile data.
      sleep_time(ms): sleep when program run this point. Default is 1 ms
    """
    return torch_gcu._GCUC._add_trace_point(trace_info, sleep_time)


def metrics_report() -> str:
    """Retrieves a string containing the full metrics and counters report."""
    return torch_gcu._GCUC._metrics_report()


def get_graph(tensor: Union[torch.Tensor, list, tuple], device: _device_t = None) -> str:
    """Return a calculate graph from given tensors.

    Args:
      tensor: gcu tensor or list/tuple of gcu tensors or empty
      device: Only used when tensor is empty. Create a graph using all lived tensors on device.
              If device is None, use current device.
    """
    _assert_type_valid(tensor, (torch.Tensor, list, tuple))
    if _is_gcu_tensor(tensor):
        tensor = [tensor]
    elif _is_gcu_tensor_list_or_tuple(tensor):
        pass
    else:
        raise TypeError(
            "tensor must be gcu tensor or list/tuple or gcu tensors, got {}".format(type(tensor)))
    device_id = _get_device_index(device, True)
    _check_device_valid(device_id)
    return torch_gcu._GCUC._get_graph(tensor, device_id)


def get_hbm_alloc_size(device: _device_t = None):
    """Get hbm allocd number by torch_gcu

    Args:
      device_id : device id, must in [0, device_count-1].
                  If device is None, use current device
    """
    device_id = _get_device_index(device, True)
    _check_device_valid(device_id)
    return torch_gcu._GCUC._get_hbm_alloc_size(device_id)
